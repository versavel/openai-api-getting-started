{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f977ebe3-1e5b-4e1c-bb1b-1869e7760e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d3c992-edea-4a40-aa17-f6544f3b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('key', 'r') as file:\n",
    "    openai_key = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1237e1b2-9891-4c24-be53-fb8cffa803ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7cffa3-57fd-4c4d-8ac2-8d95b734f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import Model, ModelDeleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4746b8b3-1f55-4075-9599-5f9d7d7f34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c91f725-0dbd-4152-a946-cea6d939a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-vision-preview\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4\n",
      "gpt-4-0613\n",
      "gpt-3.5-turbo-16k-0613\n"
     ]
    }
   ],
   "source": [
    "for k in models:\n",
    "    if \"gpt\" in k.id:\n",
    "        print(k.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e99ffa-21b9-4f4e-af1b-08d3851b3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"gpt-4-turbo-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b90907-95ee-4f6f-b28b-06fd3fb61765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19de70-ed80-4e66-b0e9-7e68914c9dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd87e8bb-f526-402f-a88a-a933c15c4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "# this function is used to blend the images together\n",
    "# lets allow the user to choose between 2 different blending methods:\n",
    "# 1. use an avergae value per-pixel to blend the images together\n",
    "# 2. use the distance-transform mask to blend the images together\n",
    "def stitch_images(imgs_Ts_and_masks, blend_method=0):\n",
    "    num_imgs = len(imgs_Ts_and_masks)\n",
    "\n",
    "    # get the center image and its (potential) mask\n",
    "    img_center = imgs_Ts_and_masks[0][0]\n",
    "    img_center_mask = imgs_Ts_and_masks[0][2]\n",
    "\n",
    "    # create a \"big\" image to show all the images on top of each other\n",
    "    # the big image should have the same height as the center image\n",
    "    # the big image should have the width of the center image multiplied by the number of images\n",
    "    # the big image should have 3 channels (RGB, np.uint8), filled with 0s\n",
    "    # the big image should be a 4D array (e.g. `np.zeros((num_imgs, img_center.shape[0], img_center.shape[1]*3, 3), dtype=np.uint8)`)\n",
    "    # we will use a 4th dimension (the 0/first dimension) to store all the images, so that we can blend them later\n",
    "    big_img = ...\n",
    "    # create a big mask to use for the blending\n",
    "    # the big mask should have the same height as the center image\n",
    "    # the big mask should have the width of the center image multiplied by the number of images\n",
    "    # the big mask should have 1 channel (np.float32), filled with 0s\n",
    "    # the big mask should be a 3D array (e.g. `np.zeros(big_img.shape[:-1], dtype=np.float32)`)\n",
    "    big_mask = ...\n",
    "\n",
    "    # add the center image to the big image (in the img_center.shape[1]:img_center.shape[1]*2 x-axis range, in the 0/first place)\n",
    "    big_img[...] = img_center\n",
    "    \n",
    "    # add the center image mask to the big mask\n",
    "    # create a mask for the center image\n",
    "    if blend_method == 0:\n",
    "        # use a simple mask with 1s, in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        big_mask[...] = 1\n",
    "    if blend_method == 1:\n",
    "        # use a distance transform mask, using the function `create_distance_transform_mask` defined above\n",
    "        # and place it in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        mask = create_distance_transform_mask(img_center)\n",
    "        big_mask[...] = mask\n",
    "    \n",
    "    # if the center image has a mask, multiply the mask with the big mask\n",
    "    if img_center_mask is not None:\n",
    "        # multiply the mask with the big mask in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        # (use the *= operator) if the center image has a mask\n",
    "        big_mask[...] *= img_center_mask\n",
    "\n",
    "    # add the other images to the big image\n",
    "    # for each image-and-transform tuple (im, H, im_mask) in imgs_Ts_and_masks[1:] (skip the first one, which is the center image)\n",
    "    #   apply the homography H to the image im\n",
    "    #   add the image to the big image in the img_center.shape[1]*l:img_center.shape[1]*(l+1) x-axis range\n",
    "    #   (use the l variable to keep track of the current image index)\n",
    "    #   apply the homography H to the image mask im_mask\n",
    "    #   add the image mask to the big mask in the img_center.shape[1]*l:img_center.shape[1]*(l+1) x-axis range\n",
    "    #   if the image has a mask, multiply the mask with the big mask\n",
    "\n",
    "    for l, (im, H, im_mask) in enumerate(imgs_Ts_and_masks[1:]):\n",
    "        # we need to shift the homography to the right by img_center.shape[1]*l\n",
    "        # use the np.matmul function to multiply the homography with a simple translation matrix\n",
    "        # use the np.array function to create the translation matrix\n",
    "        # the translation matrix should be of shape (3,3) with 1s on the diagonal\n",
    "        # and -img_center.shape[1]*l in the last row, first column (for a translation in the x-axis)\n",
    "        H = ...\n",
    "\n",
    "        # create a mask for the image, same size as the image, similar to the code above\n",
    "        # use the blend_method variable (0 or 1) to determine which mask to use: 0 = simple mask, 1 = distance transform mask\n",
    "        if blend_method == 0:\n",
    "            # use a simple mask with 1s (np.ones) of the same size as the image (im.shape), dtype=np.float32\n",
    "            mask = ...\n",
    "        elif blend_method == 1:\n",
    "            # use a distance transform mask, using the function `create_distance_transform_mask` defined above\n",
    "            mask = ...\n",
    "\n",
    "        # add the mask to the big mask, same as above\n",
    "        if im_mask is not None:\n",
    "            mask *= im_mask\n",
    "\n",
    "        # apply the homography to the mask by using the cv2.warpPerspective function\n",
    "        # the shape of the output should be (big_mask.shape[1], big_mask.shape[2])\n",
    "        # the output should be the big_mask[l+1] (the l+1-th image in \"big mask\")\n",
    "        # remember to use the borderMode=cv2.BORDER_TRANSPARENT parameter to avoid black borders\n",
    "        big_mask[l+1] = ...\n",
    "\n",
    "        # apply the homography to the image by using the cv2.warpPerspective function\n",
    "        # the shape of the output should be (big_img.shape[1], big_img.shape[2])\n",
    "        big_img[l+1] = ...\n",
    "    \n",
    "    # save the original big mask (before normalization) for visualization later\n",
    "    big_mask_orig = big_mask.copy()\n",
    "\n",
    "    # normalize the mask (divide by the sum of the mask)\n",
    "    big_mask_sum = np.sum(big_mask, axis=0)\n",
    "    for i in range(num_imgs):\n",
    "        # divide the big_mask[i] by the sum of the mask (big_mask_sum), but only for the pixels that are > 0 (use the big_mask[i] > 0 condition)\n",
    "        big_mask[i][...] = ...\n",
    "\n",
    "    # blend the images using the mask\n",
    "    big_img_out = np.zeros(big_img.shape[1:], dtype=np.float32)\n",
    "    for i in range(num_imgs):\n",
    "        # multiply the image (big_img[i]) with the mask (big_mask[i]) and add it to the big_img_out\n",
    "        # use the np.repeat function to repeat the mask 3 times (for the 3 color channels, RGB)\n",
    "        # the trick is to use big_mask[i][...,None] and axis=2 for np.repeat to get the right shape\n",
    "        # use the np.float32 type for the big_mask[i] and the big_img[i]\n",
    "        big_img_out += ...\n",
    "\n",
    "    # convert the big_img_out to uint8 (with `.astype(np.uint8)`)\n",
    "    return big_img_out.astype(np.uint8), big_mask, big_mask_orig\n",
    "\n",
    "\n",
    "# run the function with the images and the transforms and inliers from the previous cell\n",
    "# lets use imgs[1] as the center image, and imgs[0] as the left image, imgs[2] as the right image, and imgs[3] as the last image\n",
    "# imgs[1] is the center image, so we don't need to apply any transform to it, so we use the identity matrix (np.eye(3))\n",
    "# imgs[0] is the left image, so we need to apply the transform transforms_and_inliers[0][1][0] (0->1) to it\n",
    "# imgs[2] is the right image, so we need to apply the inverse of the transform transforms_and_inliers[1][2][0] to it (np.linalg.inv, to get 2->1 transform)\n",
    "# imgs[3] is the last image, so we need to apply the inverse of the transform transforms_and_inliers[1][2][0] (2->1) and transforms_and_inliers[2][3][0] (3->2) to it\n",
    "#   we can use the np.matmul function to multiply the two transforms and then use np.linalg.inv to get the overall inverse (3->2->1 = 3->1)\n",
    "big_img, big_mask, big_mask_orig = stitch_images([\n",
    "        (imgs[1], ...),\n",
    "        (imgs[0], ...),\n",
    "        (imgs[2], ...),\n",
    "        (imgs[3], ...),\n",
    "    ],\n",
    "    blend_method=1\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628fbb66-fae7-488c-897f-5e6173863039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your script outlines a basic approach to stitching images together to create a panorama, but there are several key parts missing or requiring clarification for it to work. Here's a step-by-step guide to fill in the gaps and correct some misunderstandings in your approach:\n",
      "\n",
      "### 1. Preparing the Big Image and Big Mask Arrays\n",
      "\n",
      "Your description of creating a \"big\" image and mask with a 4th dimension to store all images for later blending is not practical for image stitching. Instead, you should aim to create a large canvas where all transformed images can be placed according to their relative positions. This canvas should initially be large enough to hold all images once transformed and placed relative to each other. The exact size can be determined after applying transformations and calculating the bounding box that can contain all images.\n",
      "\n",
      "### 2. Applying Homographies and Blending\n",
      "\n",
      "You need to warp each image (except the reference image) using the corresponding homography matrix to align it with the reference image. After warping, you can blend the images together. For simplicity, let's focus on creating a large enough canvas and placing images on it.\n",
      "\n",
      "### 3. Correcting the Approach\n",
      "\n",
      "- **Creating a large canvas**: Instead of a 4D array, create a 2D canvas large enough to hold all the transformed images. You'll need to calculate the size after applying transformations to all images to find the bounding box.\n",
      "- **Applying transformations**: Use `cv2.warpPerspective` to apply the homography to each image and its mask. You'll need to adjust the homography to account for the offset in the large canvas.\n",
      "- **Blending**: For blending, you can average overlapping pixels or use more sophisticated methods like multi-band blending for smoother results.\n",
      "\n",
      "### 4. Sample Code Adjustments\n",
      "\n",
      "Here's a simplified approach focusing on key steps:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "def stitch_images(images, homographies):\n",
      "    # Assuming 'images' is a list of images and 'homographies' is a list of corresponding homography matrices\n",
      "    # with the first image being the reference (identity matrix as its homography)\n",
      "\n",
      "    # Placeholder for canvas size calculation\n",
      "    # In practice, you should calculate the size by applying transformations and finding the bounding box\n",
      "\n",
      "    # For simplicity, let's assume a fixed size canvas that's large enough\n",
      "    canvas_width = 3000\n",
      "    canvas_height = 1000\n",
      "    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n",
      "\n",
      "    for i, (image, H) in enumerate(zip(images, homographies)):\n",
      "        # Warp image\n",
      "        warped_image = cv2.warpPerspective(image, H, (canvas_width, canvas_height))\n",
      "        # Place image on canvas\n",
      "        # This is a simplified approach; you should actually blend pixels in overlapping areas\n",
      "        mask = warped_image > 0\n",
      "        canvas[mask] = warped_image[mask]\n",
      "\n",
      "    return canvas\n",
      "\n",
      "# Example usage\n",
      "# images = [img1, img2, img3] # Your images here\n",
      "# homographies = [H1, H2, H3] # Your calculated homographies here\n",
      "# result = stitch_images(images, homographies)\n",
      "# cv2.imshow('Stitched Image', result)\n",
      "# cv2.waitKey(0)\n",
      "# cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "### Key Points to Remember\n",
      "\n",
      "- **Homography Adjustment**: When placing images on the canvas, you might need to adjust the homography matrices to account for the translation within the canvas.\n",
      "- **Blending**: The provided code does not implement blending. You'll need to handle overlapping areas more gracefully than simply overwriting pixels.\n",
      "- **Canvas Size**: The canvas size is assumed to be large enough in the example. In practice, you should calculate it based on the transformed image positions.\n",
      "\n",
      "This example simplifies many aspects of panorama stitching, focusing on key concepts. For a complete solution, consider handling edge cases, dynamically calculating the canvas size, and implementing advanced blending techniques.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "I'm coding in python and using the OpenCV library. I want a python script to build panoramas using the following simple algorithm\n",
    "step 1: extract keypoints and descriptors from all images\n",
    "step 2: find matches between all images\n",
    "step 3: select a reference image\n",
    "step 4: begin adding the other images together with the reference image by\n",
    "  by first finding a transformation between them\n",
    "  and then blending the images together\n",
    "\n",
    "Following is the script I that I currently have. I want you to give me feedback on how I could make it work.\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa09e5-a48d-49ba-8e6c-a782d1077183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
