{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f977ebe3-1e5b-4e1c-bb1b-1869e7760e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d3c992-edea-4a40-aa17-f6544f3b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('key', 'r') as file:\n",
    "    openai_key = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1237e1b2-9891-4c24-be53-fb8cffa803ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7cffa3-57fd-4c4d-8ac2-8d95b734f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import Model, ModelDeleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4746b8b3-1f55-4075-9599-5f9d7d7f34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b5edd0-8a62-409e-8006-880d8390e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.pagination.SyncPage[Model]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c91f725-0dbd-4152-a946-cea6d939a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-vision-preview\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-4\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0613\n"
     ]
    }
   ],
   "source": [
    "for k in models:\n",
    "    if \"gpt-4\" in k.id:\n",
    "        print(k.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866deceb-83a7-40b4-9549-6563d25bdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in models:\n",
    "    if \"gpt-5\" in k.id:\n",
    "        print(k.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b90907-95ee-4f6f-b28b-06fd3fb61765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19de70-ed80-4e66-b0e9-7e68914c9dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628fbb66-fae7-488c-897f-5e6173863039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your script seems to be on the right track, but there are several places where you've left placeholders (e.g. `...`) where you need to fill in the actual code. Here are some suggestions on how to complete your script:\n",
      "\n",
      "1. For creating the `big_img` and `big_mask`, you can use `np.zeros` function. For example:\n",
      "```python\n",
      "big_img = np.zeros((num_imgs, img_center.shape[0], img_center.shape[1]*num_imgs, 3), dtype=np.uint8)\n",
      "big_mask = np.zeros((num_imgs, img_center.shape[0], img_center.shape[1]*num_imgs), dtype=np.float32)\n",
      "```\n",
      "\n",
      "2. For adding the center image to the `big_img` and `big_mask`, you can use slicing. For example:\n",
      "```python\n",
      "big_img[0, :, img_center.shape[1]:img_center.shape[1]*2] = img_center\n",
      "big_mask[0, :, img_center.shape[1]:img_center.shape[1]*2] = 1 if blend_method == 0 else create_distance_transform_mask(img_center)\n",
      "```\n",
      "\n",
      "3. For applying the homography `H` to the image `im` and the mask, you can use `cv2.warpPerspective` function. For example:\n",
      "```python\n",
      "big_mask[l+1] = cv2.warpPerspective(mask, H, (big_mask.shape[1], big_mask.shape[2]), borderMode=cv2.BORDER_TRANSPARENT)\n",
      "big_img[l+1] = cv2.warpPerspective(im, H, (big_img.shape[1], big_img.shape[2]))\n",
      "```\n",
      "\n",
      "4. For normalizing the mask, you can use the `/=` operator. For example:\n",
      "```python\n",
      "big_mask[i][big_mask[i] > 0] /= big_mask_sum[big_mask[i] > 0]\n",
      "```\n",
      "\n",
      "5. For blending the images using the mask, you can use the `*` operator. For example:\n",
      "```python\n",
      "big_img_out += big_img[i] * np.repeat(big_mask[i][...,None], 3, axis=2)\n",
      "```\n",
      "\n",
      "6. For the last part of your script where you're calling the `stitch_images` function, you need to replace the `...` with the actual transformations. For example:\n",
      "```python\n",
      "big_img, big_mask, big_mask_orig = stitch_images([\n",
      "    (imgs[1], np.eye(3)),\n",
      "    (imgs[0], transforms_and_inliers[0][1][0]),\n",
      "    (imgs[2], np.linalg.inv(transforms_and_inliers[1][2][0])),\n",
      "    (imgs[3], np.linalg.inv(np.matmul(transforms_and_inliers[1][2][0], transforms_and_inliers[2][3][0]))),\n",
      "], blend_method=1)\n",
      "```\n",
      "\n",
      "Please note that this is just a rough guide and you may need to adjust the code to fit your specific needs. Also, make sure to handle any potential errors or exceptions that may occur during the execution of your script.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "I'm coding in python and using the OpenCV library. I want a python script to build panoramas using the following simple algorithm\n",
    "step 1: extract keypoints and descriptors from all images\n",
    "step 2: find matches between all images\n",
    "step 3: select a reference image\n",
    "step 4: begin adding the other images together with the reference image by\n",
    "  by first finding a transformation between them\n",
    "  and then blending the images together\n",
    "\n",
    "Following is the script I that I currently have. I want you to give me feedback on how I could make it work.\n",
    "\n",
    "# this function is used to blend the images together\n",
    "# lets allow the user to choose between 2 different blending methods:\n",
    "# 1. use an avergae value per-pixel to blend the images together\n",
    "# 2. use the distance-transform mask to blend the images together\n",
    "def stitch_images(imgs_Ts_and_masks, blend_method=0):\n",
    "    num_imgs = len(imgs_Ts_and_masks)\n",
    "\n",
    "    # get the center image and its (potential) mask\n",
    "    img_center = imgs_Ts_and_masks[0][0]\n",
    "    img_center_mask = imgs_Ts_and_masks[0][2]\n",
    "\n",
    "    # create a \"big\" image to show all the images on top of each other\n",
    "    # the big image should have the same height as the center image\n",
    "    # the big image should have the width of the center image multiplied by the number of images\n",
    "    # the big image should have 3 channels (RGB, np.uint8), filled with 0s\n",
    "    # the big image should be a 4D array (e.g. `np.zeros((num_imgs, img_center.shape[0], img_center.shape[1]*3, 3), dtype=np.uint8)`)\n",
    "    # we will use a 4th dimension (the 0/first dimension) to store all the images, so that we can blend them later\n",
    "    big_img = ...\n",
    "    # create a big mask to use for the blending\n",
    "    # the big mask should have the same height as the center image\n",
    "    # the big mask should have the width of the center image multiplied by the number of images\n",
    "    # the big mask should have 1 channel (np.float32), filled with 0s\n",
    "    # the big mask should be a 3D array (e.g. `np.zeros(big_img.shape[:-1], dtype=np.float32)`)\n",
    "    big_mask = ...\n",
    "\n",
    "    # add the center image to the big image (in the img_center.shape[1]:img_center.shape[1]*2 x-axis range, in the 0/first place)\n",
    "    big_img[...] = img_center\n",
    "    \n",
    "    # add the center image mask to the big mask\n",
    "    # create a mask for the center image\n",
    "    if blend_method == 0:\n",
    "        # use a simple mask with 1s, in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        big_mask[...] = 1\n",
    "    if blend_method == 1:\n",
    "        # use a distance transform mask, using the function `create_distance_transform_mask` defined above\n",
    "        # and place it in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        mask = create_distance_transform_mask(img_center)\n",
    "        big_mask[...] = mask\n",
    "    \n",
    "    # if the center image has a mask, multiply the mask with the big mask\n",
    "    if img_center_mask is not None:\n",
    "        # multiply the mask with the big mask in the img_center.shape[1]:img_center.shape[1]*2 x-axis range\n",
    "        # (use the *= operator) if the center image has a mask\n",
    "        big_mask[...] *= img_center_mask\n",
    "\n",
    "    # add the other images to the big image\n",
    "    # for each image-and-transform tuple (im, H, im_mask) in imgs_Ts_and_masks[1:] (skip the first one, which is the center image)\n",
    "    #   apply the homography H to the image im\n",
    "    #   add the image to the big image in the img_center.shape[1]*l:img_center.shape[1]*(l+1) x-axis range\n",
    "    #   (use the l variable to keep track of the current image index)\n",
    "    #   apply the homography H to the image mask im_mask\n",
    "    #   add the image mask to the big mask in the img_center.shape[1]*l:img_center.shape[1]*(l+1) x-axis range\n",
    "    #   if the image has a mask, multiply the mask with the big mask\n",
    "\n",
    "    for l, (im, H, im_mask) in enumerate(imgs_Ts_and_masks[1:]):\n",
    "        # we need to shift the homography to the right by img_center.shape[1]*l\n",
    "        # use the np.matmul function to multiply the homography with a simple translation matrix\n",
    "        # use the np.array function to create the translation matrix\n",
    "        # the translation matrix should be of shape (3,3) with 1s on the diagonal\n",
    "        # and -img_center.shape[1]*l in the last row, first column (for a translation in the x-axis)\n",
    "        H = ...\n",
    "\n",
    "        # create a mask for the image, same size as the image, similar to the code above\n",
    "        # use the blend_method variable (0 or 1) to determine which mask to use: 0 = simple mask, 1 = distance transform mask\n",
    "        if blend_method == 0:\n",
    "            # use a simple mask with 1s (np.ones) of the same size as the image (im.shape), dtype=np.float32\n",
    "            mask = ...\n",
    "        elif blend_method == 1:\n",
    "            # use a distance transform mask, using the function `create_distance_transform_mask` defined above\n",
    "            mask = ...\n",
    "\n",
    "        # add the mask to the big mask, same as above\n",
    "        if im_mask is not None:\n",
    "            mask *= im_mask\n",
    "\n",
    "        # apply the homography to the mask by using the cv2.warpPerspective function\n",
    "        # the shape of the output should be (big_mask.shape[1], big_mask.shape[2])\n",
    "        # the output should be the big_mask[l+1] (the l+1-th image in \"big mask\")\n",
    "        # remember to use the borderMode=cv2.BORDER_TRANSPARENT parameter to avoid black borders\n",
    "        big_mask[l+1] = ...\n",
    "\n",
    "        # apply the homography to the image by using the cv2.warpPerspective function\n",
    "        # the shape of the output should be (big_img.shape[1], big_img.shape[2])\n",
    "        big_img[l+1] = ...\n",
    "    \n",
    "    # save the original big mask (before normalization) for visualization later\n",
    "    big_mask_orig = big_mask.copy()\n",
    "\n",
    "    # normalize the mask (divide by the sum of the mask)\n",
    "    big_mask_sum = np.sum(big_mask, axis=0)\n",
    "    for i in range(num_imgs):\n",
    "        # divide the big_mask[i] by the sum of the mask (big_mask_sum), but only for the pixels that are > 0 (use the big_mask[i] > 0 condition)\n",
    "        big_mask[i][...] = ...\n",
    "\n",
    "    # blend the images using the mask\n",
    "    big_img_out = np.zeros(big_img.shape[1:], dtype=np.float32)\n",
    "    for i in range(num_imgs):\n",
    "        # multiply the image (big_img[i]) with the mask (big_mask[i]) and add it to the big_img_out\n",
    "        # use the np.repeat function to repeat the mask 3 times (for the 3 color channels, RGB)\n",
    "        # the trick is to use big_mask[i][...,None] and axis=2 for np.repeat to get the right shape\n",
    "        # use the np.float32 type for the big_mask[i] and the big_img[i]\n",
    "        big_img_out += ...\n",
    "\n",
    "    # convert the big_img_out to uint8 (with `.astype(np.uint8)`)\n",
    "    return big_img_out.astype(np.uint8), big_mask, big_mask_orig\n",
    "\n",
    "\n",
    "# run the function with the images and the transforms and inliers from the previous cell\n",
    "# lets use imgs[1] as the center image, and imgs[0] as the left image, imgs[2] as the right image, and imgs[3] as the last image\n",
    "# imgs[1] is the center image, so we don't need to apply any transform to it, so we use the identity matrix (np.eye(3))\n",
    "# imgs[0] is the left image, so we need to apply the transform transforms_and_inliers[0][1][0] (0->1) to it\n",
    "# imgs[2] is the right image, so we need to apply the inverse of the transform transforms_and_inliers[1][2][0] to it (np.linalg.inv, to get 2->1 transform)\n",
    "# imgs[3] is the last image, so we need to apply the inverse of the transform transforms_and_inliers[1][2][0] (2->1) and transforms_and_inliers[2][3][0] (3->2) to it\n",
    "#   we can use the np.matmul function to multiply the two transforms and then use np.linalg.inv to get the overall inverse (3->2->1 = 3->1)\n",
    "big_img, big_mask, big_mask_orig = stitch_images([\n",
    "        (imgs[1], ...),\n",
    "        (imgs[0], ...),\n",
    "        (imgs[2], ...),\n",
    "        (imgs[3], ...),\n",
    "    ],\n",
    "    blend_method=1\n",
    "    )\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa09e5-a48d-49ba-8e6c-a782d1077183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
