{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f977ebe3-1e5b-4e1c-bb1b-1869e7760e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d3c992-edea-4a40-aa17-f6544f3b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('key', 'r') as file:\n",
    "    openai_key = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1237e1b2-9891-4c24-be53-fb8cffa803ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7cffa3-57fd-4c4d-8ac2-8d95b734f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import Model, ModelDeleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4746b8b3-1f55-4075-9599-5f9d7d7f34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b5edd0-8a62-409e-8006-880d8390e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.pagination.SyncPage[Model]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c91f725-0dbd-4152-a946-cea6d939a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-vision-preview\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0613\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-16k\n"
     ]
    }
   ],
   "source": [
    "for k in models:\n",
    "    if \"gpt\" in k.id:\n",
    "        print(k.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b90907-95ee-4f6f-b28b-06fd3fb61765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e19de70-ed80-4e66-b0e9-7e68914c9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "step 1: extract keypoints and descriptors from all images\n",
    "step 2: find matches between all images\n",
    "step 3: select a reference image\n",
    "step 4: begin adding the other images together with the reference image by\n",
    "  first finding a transformation between them\n",
    "  and then blending the images together\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "628fbb66-fae7-488c-897f-5e6173863039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple script that follows the steps you've outlined using OpenCV and Python:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "\n",
      "def stitch_images(images):\n",
      "    # Step 1: Detect keypoints and extract local invariant descriptors\n",
      "    sift = cv2.xfeatures2d.SIFT_create()\n",
      "    keypoints = []\n",
      "    descriptors = []\n",
      "    for image in images:\n",
      "        keypoint, descriptor = sift.detectAndCompute(image, None)\n",
      "        keypoints.append(keypoint)\n",
      "        descriptors.append(descriptor)\n",
      "\n",
      "    # Step 2: Match descriptors between images\n",
      "    matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
      "    raw_matches = matcher.knnMatch(descriptors[0], descriptors[1], 2)\n",
      "\n",
      "    # Lowe's ratio test\n",
      "    good_points = []\n",
      "    for m, n in raw_matches:\n",
      "        if m.distance < 0.75 * n.distance:\n",
      "            good_points.append(m)\n",
      "\n",
      "    # Step 3: Select a reference image\n",
      "    # In this case, we'll just use the first image as the reference\n",
      "    ref_image = images[0]\n",
      "\n",
      "    # Step 4: Find a transformation between the reference image and the other images\n",
      "    src_pts = np.float32([ keypoints[0][m.queryIdx].pt for m in good_points ]).reshape(-1,1,2)\n",
      "    dst_pts = np.float32([ keypoints[1][m.trainIdx].pt for m in good_points ]).reshape(-1,1,2)\n",
      "\n",
      "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
      "\n",
      "    # Step 5: Blend the images together\n",
      "    result = cv2.warpPerspective(ref_image, M, (images[1].shape[1] + images[0].shape[1], images[0].shape[0]))\n",
      "    result[0:images[0].shape[0], 0:images[0].shape[1]] = images[0]\n",
      "\n",
      "    return result\n",
      "\n",
      "# Load images\n",
      "image1 = cv2.imread('image1.jpg')\n",
      "image2 = cv2.imread('image2.jpg')\n",
      "\n",
      "# Stitch images\n",
      "result = stitch_images([image1, image2])\n",
      "\n",
      "# Display result\n",
      "cv2.imshow('Stitched Image', result)\n",
      "cv2.waitKey(0)\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "This script assumes that you have two images, 'image1.jpg' and 'image2.jpg', in the same directory as the script. It uses SIFT for keypoint detection and descriptor extraction, and Brute-Force matcher to match these descriptors. The first image is used as the reference image. The script then finds a homography between the reference image and the other images, and finally blends the images together. The result is displayed in a window.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "I'm coding in python and using the OpenCV library. Give me a python script to build panoramas using the following algorithm:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa09e5-a48d-49ba-8e6c-a782d1077183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
